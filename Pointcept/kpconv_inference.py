#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ KPConv –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–º .pcd —Ñ–∞–π–ª–µ
–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ü–≤–µ—Ç–∞–º–∏ S3DIS
"""

import os
import sys
import time
import numpy as np
import torch
import open3d as o3d
from torch.nn.functional import softmax
import argparse

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ –º–æ–¥—É–ª—è–º
sys.path.append('./third_party/KPConv-PyTorch')
sys.path.append('./third_party/sonata')

# –ò–º–ø–æ—Ä—Ç—ã KPConv
from utils.config import Config
from models.architectures import KPFCNN
from datasets.common import PointCloudDataset
from datasets.S3DIS import S3DISCustomBatch
import random

# –ò–º–ø–æ—Ä—Ç—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
from pcd_preprocessor import load_and_preprocess_pcd, convert_to_kpconv_format


def parse_args():
    parser = argparse.ArgumentParser(description="KPConv Semantic Segmentation Inference")
    parser.add_argument('--pcd_path', type=str, default='/workspace/pcd_files/down0.01.pcd',
                        help='Path to the input PCD file.')
    parser.add_argument('--model_path', type=str, default='/workspace/kpconv_weights/Light_KPFCNN',
                        help='Path to the directory containing the trained KPConv model.')
    parser.add_argument('--output_dir', type=str, default='result_plys/kpconv_plys',
                        help='Directory to save the output PLY file.')
    parser.add_argument('--downsampling_method', type=str, default='grid', choices=['grid', 'voxel', 'random'],
                        help='Downsampling method.')
    parser.add_argument('--voxel_size', type=float, default=0.03,
                        help='Voxel size for downsampling.')
    parser.add_argument('--chunk_size', type=int, default=300000,
                        help='Number of points to process in a single chunk.')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed for reproducibility.')
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'],
                        help='Device to use for inference.')
    return parser.parse_args()


def set_random_seed(seed=42):
    """
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç random seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    print(f"üé≤ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ random seed: {seed}")
    
    # Python random
    random.seed(seed)
    
    # NumPy
    np.random.seed(seed)
    
    # PyTorch
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # –î–ª—è multi-GPU
    
    # –î–ª—è –ø–æ–ª–Ω–æ–π –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ (–º–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª–∏—Ç—å —Ä–∞–±–æ—Ç—É)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print("‚úÖ Random seed —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –≤—Å–µ—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫")


class CustomPointCloudDataset(PointCloudDataset):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –æ–¥–Ω–æ–≥–æ –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫"""
    
    def __init__(self, config):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–ª–∞—Å—Å –±–µ–∑ –∏–º–µ–Ω–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
        super().__init__('Custom')
        self.config = config
        
        # S3DIS –∫–ª–∞—Å—Å—ã
        self.label_to_names = {
            0: 'ceiling',
            1: 'floor',
            2: 'wall',
            3: 'beam',
            4: 'column',
            5: 'window',
            6: 'door',
            7: 'chair',
            8: 'table',
            9: 'bookcase',
            10: 'sofa',
            11: 'board',
            12: 'clutter',
        }
        self.init_labels()
        self.ignored_labels = np.array([])


class KPConvInferencer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ KPConv –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, model_path, device='cuda'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ—Ä–∞
        
        Args:
            model_path (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
            device (str): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ('cuda' –∏–ª–∏ 'cpu')
        """
        self.model_path = model_path
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.config = Config()
        self.config.load(model_path)
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏: {self.config.dataset}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {self.config.num_classes}")
        print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {self.config.in_features_dim}")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤
        self.dataset = CustomPointCloudDataset(self.config)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self._load_model()
        
        # # –¶–≤–µ—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ S3DIS (RGB –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1)
        # self.class_colors = np.array([
        #     [0.7, 0.7, 0.7],   # 0: ceiling - —Å–µ—Ä—ã–π
        #     [0.5, 0.3, 0.1],   # 1: floor - –∫–æ—Ä–∏—á–Ω–µ–≤—ã–π
        #     [0.8, 0.8, 0.6],   # 2: wall - –±–µ–∂–µ–≤—ã–π
        #     [0.4, 0.2, 0.0],   # 3: beam - —Ç–µ–º–Ω–æ-–∫–æ—Ä–∏—á–Ω–µ–≤—ã–π
        #     [0.6, 0.6, 0.6],   # 4: column - —Ç–µ–º–Ω–æ-—Å–µ—Ä—ã–π
        #     [0.0, 0.5, 0.8],   # 5: window - –≥–æ–ª—É–±–æ–π
        #     [0.8, 0.4, 0.0],   # 6: door - –æ—Ä–∞–Ω–∂–µ–≤—ã–π
        #     [0.0, 0.8, 0.0],   # 7: chair - –∑–µ–ª–µ–Ω—ã–π
        #     [0.8, 0.0, 0.0],   # 8: table - –∫—Ä–∞—Å–Ω—ã–π
        #     [0.4, 0.0, 0.8],   # 9: bookcase - —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π
        #     [0.8, 0.0, 0.8],   # 10: sofa - –ø—É—Ä–ø—É—Ä–Ω—ã–π
        #     [0.0, 0.0, 0.8],   # 11: board - —Å–∏–Ω–∏–π
        #     [0.8, 0.8, 0.0],   # 12: clutter - –∂–µ–ª—Ç—ã–π
        # ])
        self.class_colors = np.array([
            [82/255, 84/255, 163/255],    # 0: ceiling -> otherfurniture
            [152/255, 223/255, 138/255],    # 1: floor -> floor
            [174/255, 199/255, 232/255],    # 2: wall -> wall
            [82/255, 84/255, 163/255],    # 3: beam -> otherfurniture
            [82/255, 84/255, 163/255],    # 4: column -> otherfurniture
            [197/255, 176/255, 213/255],     # 5: window -> window
            [214/255, 39/255, 40/255],    # 6: door -> door
            [188/255, 189/255, 34/255],    # 7: chair -> chair
            [255/255, 152/255, 150/255],    # 8: table -> table
            [140/255, 86/255, 75/255],  # 9: sofa -> sofa
            [148/255, 103/255, 189/255],  # 10: bookcase -> bookshelf
            [196/255, 156/255, 148/255],  # 11: board -> picture
            [82/255, 84/255, 163/255],  # 12: clutter/other -> otherfurniture
        ])
        


    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
        
        # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
        self.model = KPFCNN(
            self.config, 
            self.dataset.label_values, 
            self.dataset.ignored_labels
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
        checkpoint_path = os.path.join(self.model_path, 'checkpoints')
        checkpoints = [f for f in os.listdir(checkpoint_path) if f.startswith('chkp')]
        
        if not checkpoints:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã —á–µ–∫–ø–æ–∏–Ω—Ç—ã –≤ {checkpoint_path}")
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        latest_checkpoint = sorted(checkpoints)[-1]
        chkp_path = os.path.join(checkpoint_path, latest_checkpoint)
        
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç: {latest_checkpoint}")
        
        checkpoint = torch.load(chkp_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    
    def _create_batch_from_points(self, points, features):
        """
        –°–æ–∑–¥–∞–µ—Ç batch –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫
        
        Args:
            points (np.ndarray): –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ç–æ—á–µ–∫ (N, 3)
            features (np.ndarray): –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ—á–µ–∫ (N, 5)
            
        Returns:
            batch: –û–±—ä–µ–∫—Ç batch –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–∫–∏ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ)
        labels = np.zeros(len(points), dtype=np.int64)
        stack_lengths = np.array([len(points)], dtype=np.int32)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        input_list = self.dataset.segmentation_inputs(
            points, features, labels, stack_lengths
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        scales = np.array([1.0], dtype=np.float32)  # –ë–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        rots = np.eye(3, dtype=np.float32)[np.newaxis, :]  # –ë–µ–∑ –ø–æ–≤–æ—Ä–æ—Ç–∞
        cloud_inds = np.array([0], dtype=np.int32)  # –ò–Ω–¥–µ–∫—Å –æ–±–ª–∞–∫–∞
        point_inds = np.array([0], dtype=np.int32)  # –ò–Ω–¥–µ–∫—Å —Ç–æ—á–∫–∏
        input_inds = np.arange(len(points), dtype=np.int32)  # –ò–Ω–¥–µ–∫—Å—ã –≤—Å–µ—Ö —Ç–æ—á–µ–∫
        
        input_list += [scales, rots, cloud_inds, point_inds, input_inds]
        
        # –°–æ–∑–¥–∞–µ–º batch –æ–±—ä–µ–∫—Ç
        batch = S3DISCustomBatch([input_list])
        
        return batch
    
    def predict(self, points, features, chunk_size=30000):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫ —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ —á–∞—Å—Ç–∏
        
        Args:
            points (np.ndarray): –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ç–æ—á–µ–∫ (N, 3)
            features (np.ndarray): –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ—á–µ–∫ (N, 5)
            chunk_size (int): –†–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            np.ndarray: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
        """
        print("–í—ã–ø–æ–ª–Ω—è–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å...")
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("   GPU –∫—ç—à –æ—á–∏—â–µ–Ω –ø–µ—Ä–µ–¥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º")
        
        num_points = len(points)
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫: {num_points}")
        
        total_inference_time = 0.0
        
        # –ï—Å–ª–∏ —Ç–æ—á–µ–∫ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
        if num_points > chunk_size:
            print(f"–†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ {chunk_size} —Ç–æ—á–µ–∫...")
            
            all_predictions = []
            all_probs = []
            
            for i in range(0, num_points, chunk_size):
                end_idx = min(i + chunk_size, num_points)
                chunk_points = points[i:end_idx]
                chunk_features = features[i:end_idx]
                
                print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç—å {i//chunk_size + 1}: —Ç–æ—á–∫–∏ {i}-{end_idx}")
                
                # –û—á–∏—â–∞–µ–º –∫—ç—à GPU –ø–µ—Ä–µ–¥ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç—å—é
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                chunk_pred, chunk_probs, chunk_time = self._predict_chunk(chunk_points, chunk_features)
                all_predictions.append(chunk_pred)
                all_probs.append(chunk_probs)
                total_inference_time += chunk_time
            
            predictions = np.concatenate(all_predictions)
            probs = np.concatenate(all_probs)
        else:
            predictions, probs, inference_time = self._predict_chunk(points, features)
            total_inference_time = inference_time
        
        print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã –¥–ª—è {len(predictions)} —Ç–æ—á–µ–∫")
        print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {total_inference_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        return predictions, probs
    
    def _predict_chunk(self, points, features):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        # –°–æ–∑–¥–∞–µ–º batch
        batch = self._create_batch_from_points(points, features)
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if 'cuda' in str(self.device):
            batch.to(self.device)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            start_time = time.time()
            outputs = self.model(batch, self.config)
            inference_time = time.time() - start_time
            
            print(f"  –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —á–∞—Å—Ç–∏: {inference_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
        probs = softmax(outputs, dim=1).cpu().numpy()
        predictions = np.argmax(probs, axis=1)
        
        return predictions, probs, inference_time
    
    def colorize_predictions(self, points, predictions):
        """
        –°–æ–∑–¥–∞–µ—Ç —Ü–≤–µ—Ç–Ω–æ–µ –æ–±–ª–∞–∫–æ —Ç–æ—á–µ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        
        Args:
            points (np.ndarray): –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ç–æ—á–µ–∫ (N, 3)
            predictions (np.ndarray): –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (N,)
            
        Returns:
            o3d.geometry.PointCloud: –¶–≤–µ—Ç–Ω–æ–µ –æ–±–ª–∞–∫–æ —Ç–æ—á–µ–∫
        """
        # –°–æ–∑–¥–∞–µ–º –æ–±–ª–∞–∫–æ —Ç–æ—á–µ–∫
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        
        # –ù–∞–∑–Ω–∞—á–∞–µ–º —Ü–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        colors = np.zeros((len(points), 3))
        for i, pred in enumerate(predictions):
            if pred < len(self.class_colors):
                colors[i] = self.class_colors[pred]
            else:
                colors[i] = [0.5, 0.5, 0.5]  # –°–µ—Ä—ã–π –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        
        pcd.colors = o3d.utility.Vector3dVector(colors)
        
        return pcd


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"""
    args = parse_args()
    
    set_random_seed(args.seed)

    print("=" * 80)
    print("KPCONV INFERENCE - –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø")
    print("=" * 80)
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    input_filename = os.path.splitext(os.path.basename(args.pcd_path))[0]
    model_name = os.path.basename(args.model_path)
    output_filename = (f"{input_filename}_KPConv_{model_name}_"
                      f"downsample_{args.downsampling_method}_voxel{args.voxel_size}m_"
                      f"chunk{args.chunk_size}_segmented_seed_{args.seed}.ply")
    output_path = os.path.join(args.output_dir, output_filename)
    
    print(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {args.pcd_path}")
    print(f"–ú–æ–¥–µ–ª—å: {model_name} (–∏–∑ {args.model_path})")
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: downsampling={args.downsampling_method}, voxel_size={args.voxel_size}m, chunk_size={args.chunk_size}, device={args.device}")
    print(f"–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {output_path}")
    
    try:
        # 0. –û—á–∏—â–∞–µ–º GPU –∫—ç—à –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
        print("\n0. –û—á–∏—â–∞–µ–º GPU –∫—ç—à...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"   GPU –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω–æ: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print("   CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
        
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        print("\n1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PCD —Ñ–∞–π–ª...")
        point_dict = load_and_preprocess_pcd(
            file_path=args.pcd_path,
            downsampling_method=args.downsampling_method,
            voxel_size=args.voxel_size,
            add_segmentation=False
        )
        
        # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç KPConv
        print("\n2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç KPConv...")
        points, features = convert_to_kpconv_format(point_dict)
        print(f"   Points shape: {points.shape}")
        print(f"   Features shape: {features.shape}")
        
        # 3. –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ—Ä –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print("\n3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...")
        # –ü—Ä–æ–±—É–µ–º CUDA, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, —É–∫–∞–∑–∞–Ω–Ω–æ–µ –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ö
            inferencer = KPConvInferencer(args.model_path, device=args.device)
            print("\n4. –í—ã–ø–æ–ª–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é...")
            predictions, probs = inferencer.predict(points, features, chunk_size=args.chunk_size)
        except RuntimeError as e:
            # –ï—Å–ª–∏ –Ω–∞ CUDA –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ –ø–∞–º—è—Ç–∏ –∏ –±—ã–ª –≤—ã–±—Ä–∞–Ω 'cuda', –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ 'cpu'
            if "CUDA out of memory." in str(e) and args.device == 'cuda':
                print("   GPU –ø–∞–º—è—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU...")
                torch.cuda.empty_cache() # –û—á–∏—â–∞–µ–º –∫—ç—à –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º
                inferencer = KPConvInferencer(args.model_path, device='cpu')
                print("\n4. –í—ã–ø–æ–ª–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ CPU...")
                predictions, probs = inferencer.predict(points, features, chunk_size=args.chunk_size)
            else:
                raise e
        
        # 5. –°–æ–∑–¥–∞–µ–º —Ü–≤–µ—Ç–Ω–æ–µ –æ–±–ª–∞–∫–æ —Ç–æ—á–µ–∫
        print("\n5. –°–æ–∑–¥–∞–µ–º —Ü–≤–µ—Ç–Ω–æ–µ –æ–±–ª–∞–∫–æ —Ç–æ—á–µ–∫...")
        colored_pcd = inferencer.colorize_predictions(points, predictions)
        
        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print(f"\n6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ {output_path}...")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        o3d.io.write_point_cloud(output_path, colored_pcd)
        
        print("\n‚úÖ –ò–ù–§–ï–†–ï–ù–° –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ—á–µ–∫: {len(points)}")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Å–æ–≤: {len(np.unique(predictions))}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
        print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–æ–≤:")
        unique, counts = np.unique(predictions, return_counts=True)
        for class_id, count in zip(unique, counts):
            class_name = inferencer.dataset.label_to_names.get(class_id, f"unknown_{class_id}")
            percentage = count / len(predictions) * 100
            print(f"  {class_name}: {count} voxels ({percentage:.1f}%)")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
